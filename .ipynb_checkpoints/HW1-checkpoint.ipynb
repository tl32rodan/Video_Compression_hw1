{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (一) Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(input_seq, m_x, m_y, m_t, L_x=352, L_y=288, L_t=100, device=torch.device('cpu')):\n",
    "    '''\n",
    "        input_seq : numpy.ndarray(3d) or torch.Tensor(5d)\n",
    "        m_x, m_y, m_z : Scalar\n",
    "        L_x, L_y, L_z : Scalar\n",
    "    ''' \n",
    "    \n",
    "    if abs(m_x) >= L_x or abs(m_y) >= L_y or abs(m_t) >= L_t:\n",
    "        return 0\n",
    "    \n",
    "    if type(input_seq) == np.ndarray and len(input_seq.shape) == 3:\n",
    "        input_seq = torch.Tensor([[input_seq]]).type(torch.float32).to(device)\n",
    "    elif type(input_seq) == torch.Tensor and len(input_seq.shape) == 5:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    \n",
    "    inputs = input_seq[:, :, :L_t-abs(m_t),:L_y-abs(m_y),:L_x-abs(m_x)]\n",
    "    filters = input_seq[:, :, abs(m_t):,abs(m_y):,abs(m_x):]\n",
    "    #print(inputs.shape)\n",
    "    \n",
    "    #print(filters.shape)\n",
    "    #start_t = time.time()\n",
    "    \n",
    "    #C_vvv = F.conv3d(inputs, filters).view(-1)[0].cpu().numpy()    \n",
    "    C_vvv = (inputs*filters).sum().cpu().numpy()\n",
    "    \n",
    "    R_xxx = C_vvv / ((L_x-abs(m_x))*(L_y-abs(m_y))*(L_t-abs(m_t)))\n",
    "    \n",
    "    #end_t = time.time()\n",
    "    #print('Autocorrelation value = ',R_xxx,' ; time = ',end_t-start_t)\n",
    "    return R_xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yuv_video(yuv_filename):\n",
    "    with open(yuv_filename ,'rb') as f:\n",
    "        width, height, n_frames = 352, 288, 100\n",
    "\n",
    "        Y = []\n",
    "        U = []\n",
    "        V = []\n",
    "\n",
    "        for i in range(n_frames):\n",
    "            yuv_frame = np.frombuffer(f.read(width*height*3//2), dtype=np.uint8)\n",
    "            Y.append(np.array(yuv_frame[:width*height]).reshape(height, width))\n",
    "            U.append(np.array(yuv_frame[width*height:-width*height//4]))\n",
    "            V.append(np.array(yuv_frame[-width*height//4:]))\n",
    "\n",
    "        Y = np.array(Y)\n",
    "        U = np.array(U)\n",
    "        V = np.array(V)\n",
    "        f.close()\n",
    "    return Y,U,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_1_2(yuv_filename, output_video_name):\n",
    "    # Read video\n",
    "    Y, U, V = read_yuv_video(yuv_filename)\n",
    "    \n",
    "    # Normalize each frame with its own mean before calculating autocorrelation\n",
    "    _Y = []\n",
    "    for frame in Y:\n",
    "        _Y.append(frame-frame.mean())\n",
    "\n",
    "    Y = np.array(_Y)\n",
    "    \n",
    "    # Calculate autocorrelation ; result is stored in R_xxx\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')     \n",
    "\n",
    "    R_xxx = np.zeros([21,144,176])\n",
    "\n",
    "    input_seq = torch.Tensor([[Y]]).type(torch.float32).to(device)\n",
    "\n",
    "\n",
    "    print('Start calculating autocorrelation...')\n",
    "    for m_t in tqdm(range(10+1), desc='m_t'):\n",
    "        m_t_fill = [10+m_t, 10-m_t]\n",
    "\n",
    "        #for m_y in tqdm(range(72+1), desc='m_y', leave=False):\n",
    "        for m_y in range(72+1):\n",
    "            m_y_fill = [m_y]\n",
    "            if m_y < 72 and m_y !=0:\n",
    "                m_y_fill.append(144-m_y)\n",
    "    \n",
    "            #for m_x in tqdm(range(88+1), desc='m_x',leave=False):\n",
    "            for m_x in range(88+1):\n",
    "                m_x_fill = [m_x]\n",
    "                if m_x < 88 and m_x !=0:\n",
    "                    m_x_fill.append(176-m_x)\n",
    "            \n",
    "                #####################################\n",
    "                result = autocorrelation(input_seq, m_x-88, m_y-72, m_t-10, device=device)\n",
    "                #####################################\n",
    "\n",
    "            for x in m_x_fill:\n",
    "                for y in m_y_fill:\n",
    "                    for t in m_t_fill:\n",
    "                        R_xxx[t][y][x] = result\n",
    "    \n",
    "    \n",
    "    print('Finish calculating autocorrelation.')\n",
    "    # Normalize with R_xxx[0,0,0] ; since we have shifted R_xxx, R_xxx[10,72,88] is actually R_xxx[0,0,0]\n",
    "    _Y = R_xxx\n",
    "\n",
    "    norm = R_xxx[10,72,88]\n",
    "    _Y = (_Y/norm)*127.5+127.5\n",
    "    _Y = np.cast['uint8'](_Y)\n",
    "    \n",
    "    # Store R_xxx as an video\n",
    "    with open(output_video_name ,'wb') as f:\n",
    "\n",
    "        width, height, n_frames = 352, 288, 100\n",
    "\n",
    "        _U = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)\n",
    "        _V = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)\n",
    "\n",
    "        #_Y = R_xxx\n",
    "        for frame in _Y:\n",
    "            f.write(frame.reshape(-1).astype(np.uint8).tobytes())\n",
    "            f.write(_U.tobytes())\n",
    "            f.write(_V.tobytes())\n",
    "        f.close()\n",
    "    print('Finish writing ',output_video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start calculating autocorrelation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bea533f322845baa6e1e3038d967940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='m_t', max=11, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish calculating autocorrelation.\n",
      "Finish writing  ./AKIYO_AC.yuv\n"
     ]
    }
   ],
   "source": [
    "yuv_filename = './AKIYO_352x288_10.yuv'\n",
    "output_video_name = './AKIYO_AC.yuv'\n",
    "\n",
    "problem_1_2(yuv_filename, output_video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_motion(predicted_seq, target_seq, m_x, m_y, m_t, L_x=352, L_y=288, L_t=100):\n",
    "    '''\n",
    "        ! Incompatible to autocorrelation(), since this is not \"true\" autocorrelation calculation !\n",
    "        \n",
    "        predicted_seq, target_seq : numpy.ndarray(3d) or torch.Tensor(5d)\n",
    "            Detail :\n",
    "                1. predicted_seq is a sequence generated by motion compensation, so it's no need to silce it.\n",
    "                2. Here we need target_seq also be silced in last dimension (time dimension) already\n",
    "                \n",
    "        m_x, m_y, m_z : Scalar\n",
    "        L_x, L_y, L_z : Scalar\n",
    "    ''' \n",
    "    \n",
    "    if abs(m_x) >= L_x or abs(m_y) >= L_y or abs(m_t) >= L_t:\n",
    "        return 0\n",
    "    \n",
    "    predicted_seq = predicted_seq[:, :,    :L_t-abs(m_t),   :L_y-abs(m_y),:]\n",
    "    target_seq    = target_seq[   :, :, abs(m_t):       , abs(m_y):      ,:]\n",
    "    \n",
    "    \n",
    "    C_vvv = (predicted_seq*target_seq).sum().cpu().numpy()\n",
    "    \n",
    "    R_xxx = C_vvv / ((L_x-abs(m_x))*(L_y-abs(m_y))*(L_t-abs(m_t)))\n",
    "    \n",
    "    #end_t = time.time()\n",
    "    #print('Autocorrelation value = ',R_xxx,' ; time = ',end_t-start_t)\n",
    "    return R_xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_estimate(source_block, target_frame, residual_frame = None, unit_size=(4,4)):\n",
    "    '''\n",
    "    source_block, target_frame, residual_frame : torch.Tensor with dimention:\n",
    "        source_block : unit_size\n",
    "        target_frame : at least unit_size\n",
    "        residual_frame : same as target_frame, is (source frame - target frame)**2 \n",
    "    '''\n",
    "    \n",
    "    assert len(target_frame.shape)==2\n",
    "    \n",
    "    max_mse = np.infty\n",
    "    result_idx = (0, 0)\n",
    "    \n",
    "    height, width = target_frame.shape\n",
    "    #print(target_frame.shape)\n",
    "    for h in range(height-unit_size[0]):\n",
    "        for w in range(width-unit_size[1]):\n",
    "            if residual_frame == None:\n",
    "                current_block = target_frame[h:h+unit_size[0], w:w+unit_size[1]]\n",
    "                mse = torch.sum((current_block-source_block)**2)\n",
    "            else:\n",
    "                mse = torch.sum(residual_frame[h:h+unit_size[0], w:w+unit_size[1]])\n",
    "                \n",
    "            if mse < max_mse:\n",
    "                max_mse = mse\n",
    "                result_idx = (h, w)\n",
    "    return result_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_1_3(yuv_filename, output_video_name):\n",
    "    Y, U, V = read_yuv_video(yuv_filename)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')     \n",
    "    input_seq = torch.Tensor(Y).type(torch.float64).to(device)\n",
    "\n",
    "    L_x, L_y, L_t = 352, 288, 100\n",
    "    unit_size=(4,4) # block size ; (height, width)\n",
    "\n",
    "    # First, do motion compensation\n",
    "    predicted_frames_by_motion = {}\n",
    "\n",
    "    for i in range(11):\n",
    "        predicted_frames_by_motion[i] = {}\n",
    "\n",
    "        print(i,\":\")\n",
    "        for source_frame_idx in range(L_t -i):\n",
    "            source_frame = input_seq[source_frame_idx]\n",
    "            target_frame = input_seq[source_frame_idx + i]\n",
    "\n",
    "            predicted_frame = torch.zeros_like(source_frame).to(device)\n",
    "            motion_vector_list = []\n",
    "\n",
    "            #print(source_frame.size())\n",
    "\n",
    "            residual_frame = (source_frame-target_frame)**2\n",
    "\n",
    "            for h_base in trange(L_y//unit_size[0]):\n",
    "                for w_base in range(L_x//unit_size[1]):\n",
    "                    # Get current block from source_frame\n",
    "                    source_block = source_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],\n",
    "                                                w_base*unit_size[1] : (w_base+1)*unit_size[1]]\n",
    "                    # Determine margins\n",
    "                    l_margin, r_margin = max(w_base*unit_size[1]-16, 0) , min(w_base*unit_size[1]+15+1, L_x)\n",
    "                    u_margin, d_margin = max(h_base*unit_size[0]-16, 0) , min(h_base*unit_size[0]+15+1, L_y)\n",
    "\n",
    "                    target_subframe = target_frame[u_margin:d_margin,l_margin:r_margin]\n",
    "                    # Calculate motion compensation result\n",
    "                    motion_vector = motion_estimate(source_block = source_block,\n",
    "                                                    target_frame = target_subframe,\n",
    "                                                    residual_frame=residual_frame,\n",
    "                                                    unit_size = unit_size)\n",
    "                    predicted_block = target_subframe[motion_vector[0]:motion_vector[0]+unit_size[0], motion_vector[1]:motion_vector[1]+unit_size[1]]\n",
    "                    # Paste prediction onto its own location\n",
    "                    predicted_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],\n",
    "                                    w_base*unit_size[1] : (w_base+1)*unit_size[1]] = predicted_block\n",
    "\n",
    "                    # Record motion vector\n",
    "                    motion_vector_list.append(motion_vector)\n",
    "\n",
    "            predicted_frames_by_motion[source_frame_idx] = (predicted_frame, motion_vector_list)\n",
    "            \n",
    "    # Second, calculate autocorrelation with motion compensated frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuv_filename = './AKIYO_352x288_10.yuv'\n",
    "output_video_name = './AKIYO_AC_motion.yuv'\n",
    "problem_1_3(yuv_filename, output_video_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
