# ## 3.

# +
import numpy as np
import os
import torch
import torch.nn.functional as F
import time
from tqdm.autonotebook import tqdm
import json
import copy

from problem_1_2 import read_yuv_video


# +
def autocorrelation_motion(predicted_seq, target_seq, m_x, m_y, m_t, 
                           L_x=352, L_y=288, L_t=100,
                           device=torch.device('cpu')):
    '''
        ! Incompatible to autocorrelation(), since this is not "true" autocorrelation calculation !
        
        predicted_seq, target_seq : torch.Tensor(5d)
            Detail :
                1. predicted_seq is a sequence generated by motion compensation, 
                   so it's no need to be sliced w.r.t time.

                
        m_x, m_y, m_t : Scalar
        L_x, L_y, L_t : Scalar
    ''' 
    
    if abs(m_x) >= L_x or abs(m_y) >= L_y or abs(m_t) >= L_t:
        return 0
    
    predicted_seq = predicted_seq[:, :,        : ,         :L_y-abs(m_y),        :L_x-abs(m_x)]
    target_seq    =    target_seq[:, :,abs(m_t): , abs(m_y):            ,abs(m_x):            ]
    

#     print(predicted_seq.size())
#     print(target_seq.size())
    C_vvv = (predicted_seq*target_seq).sum().cpu().numpy()
    
    R_xxx = C_vvv / ((L_x-abs(m_x))*(L_y-abs(m_y))*(L_t-abs(m_t)))
    
    #end_t = time.time()
    #print('Autocorrelation value = ',R_xxx,' ; time = ',end_t-start_t)
    return R_xxx


# -

def find_best_fit_block(source_block, target_blocks, unit_size=(4,4), device=torch.device('cpu')):
    '''
    source_block  : torch.Tensor with dimention (unit_size)
    target_blocks : 2D array of torch.Tensor, each element has dimention (unit_size)
    '''
    
    height, width, _, _ = target_blocks.size()
    
    # Calculate MSE
    mse = torch.sum((source_block-target_blocks)**2, (2,3)).view(-1)
    
    _, min_mse_idx = torch.topk(mse, 1, largest=False)
    
    min_mse_idx = min_mse_idx.cpu().numpy()
    
    result_idx = ((min_mse_idx//width).item(), (min_mse_idx % width).item())
    #print(' min_mse_idx = ',  min_mse_idx,'; height = ',height, ' width = ',width)
    return result_idx


def motion_compensation_single_reference(source_frame, target_frame,
                                         L_x=352, L_y=288,
                                         unit_size=(4,4), 
                                         device=torch.device('cpu'),
                                         predicted_frame = None):
    
    return_result = False
    if predicted_frame == None:
        predicted_frame = torch.zeros_like(source_frame).to(device)
        return_result = True
        
    motion_vector_list = []

    # Construct list of all target blocks to reduce # of ops
    target_blocks = []
    for h in range(L_y-unit_size[0]):
        tmp = []
        for w in range(L_x-unit_size[1]):
            tmp.append(target_frame[h:h+unit_size[0], w:w+unit_size[1]])
        
        target_blocks.append(torch.stack(tmp))
    target_blocks = torch.stack(target_blocks)
    
    # Calculate motion compesation for all blocks
    for h_base in range(L_y//unit_size[0]):
        for w_base in range(L_x//unit_size[1]):
            # Get current block from source_frame
            source_block = source_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                                        w_base*unit_size[1] : (w_base+1)*unit_size[1]]
            
            
            # Determine margins
            u_margin, d_margin = max(h_base*unit_size[0]-16, 0) , min((h_base+1)*unit_size[0]+15+1, L_y)
            l_margin, r_margin = max(w_base*unit_size[1]-16, 0) , min((w_base+1)*unit_size[1]+15+1, L_x)

            target_subblocks = target_blocks[u_margin:d_margin,l_margin:r_margin]
            
            # Calculate motion compensation result
            
            #print(target_subblocks.size())
            motion_vector = find_best_fit_block(source_block = source_block,
                                                target_blocks = target_subblocks,
                                                unit_size = unit_size,
                                                device=device)
            if motion_vector[0] >= target_subblocks.size()[0] or motion_vector[1] >= target_subblocks.size()[1]:
                print('motion_vector = ',motion_vector)
                print('target_subblocks.size() = ', target_subblocks.size())
            
            predicted_block = target_subblocks[motion_vector[0],motion_vector[1],:,:][0]
                
            motion_vector = (motion_vector[0]-(h_base*unit_size[0]-u_margin), 
                             motion_vector[1]-(w_base*unit_size[1]-l_margin))

            # Paste prediction onto its own location
            predicted_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                            w_base*unit_size[1] : (w_base+1)*unit_size[1]] = predicted_block

            # Record motion vector
            motion_vector_list.append(motion_vector)
    if return_result:
        return predicted_block, motion_vector_list
    else:
        return motion_vector_list


# +
def establish_motion_compensation_dict(Y, output_video_name, dict_filename='tmp.json', 
                                       device = torch.device('cpu')):
   
    input_seq = torch.Tensor(Y).type(torch.float64).to(device)

    L_x, L_y, L_t = 352, 288, 100
    unit_size=(4,4) # block size ; (height, width)
    search_range = (16,16) # Motion compensation search range : -16~+15

    motion_result_dict_by_step = {}

    for i in tqdm(range(11), desc='m_t'):        
        motion_vector_lists = []
        predicted_frames = torch.zeros_like(input_seq[:L_t -i])
        
        for source_frame_idx in tqdm(range(L_t -i), desc='source frame index', leave=False):
            source_frame = input_seq[source_frame_idx]
            target_frame = input_seq[source_frame_idx + i]

            motion_vector_list = motion_compensation_single_reference(source_frame, target_frame, 
                                                                        L_x=L_x, L_y=L_y, 
                                                                        unit_size=unit_size,
                                                                        device=device,
                                                                        predicted_frame=predicted_frames[source_frame_idx])

            motion_vector_lists.append(motion_vector_list)
        
        
        motion_result_dict_by_step[i] = {
            'Predict_frames':predicted_frames,
            'Motion_vector_lists':motion_vector_lists
        }
        
    # Save results
    with open(dict_filename,'w') as f:
        tmp_dict = copy.deepcopy(motion_result_dict_by_step)

        for key in tmp_dict:
            tmp_dict[key]['Predict_frames'] = tmp_dict[key]['Predict_frames'].cpu().numpy().astype('int').tolist()

        json.dump(tmp_dict, f)
        f.close()
    
    return motion_result_dict_by_step        
    
    
    
# -

def run_1_3(yuv_filename, output_video_name, dict_filename='tmp.json', device=torch.device('cpu')):
    Y, U, V = read_yuv_video(yuv_filename)
    
    # ----------------------------------------------------------------
    # Normalize each frame with its own mean before calculating autocorrelation
    # ----------------------------------------------------------------
    _Y = []
    for frame in Y:
        _Y.append(frame-frame.mean())

    Y = np.array(_Y)
    
    R_xxx = np.zeros([21,144,176])

    input_seq = torch.Tensor([[Y]]).type(torch.float32).to(device)
    
    # ----------------------------------------------------------------
    # First, do motion compensation if the dict is not established
    # ----------------------------------------------------------------
    if not os.path.isfile(dict_filename): 
        # Establish new dict
        print('Start to establish new dict of motion-compensated frames...')
        motion_result_dict = establish_motion_compensation_dict(Y=Y, 
                                                                output_video_name=output_video_name, 
                                                                dict_filename=dict_filename,
                                                                device=device)
        print('Finish estiblishment')
    else:
        # Use exist dict
        
        print('Start to load exist dict of motion-compensated frames...')
        with open(dict_filename, 'r') as fp: 
            motion_result_dict = json.load(fp)
            fp.close()
        print('Finish loading')    
    # ----------------------------------------------------------------
    # Second, calculate autocorrelation with motion compensated frames
    # ----------------------------------------------------------------
    print('Start calculating autocorrelation...')
    for m_t in tqdm(range(10+1), desc='m_t'):
        
        # Get pre-calculated compensated frames
        predicted_seq = motion_result_dict[str(abs(m_t-10))]['Predict_frames']
        predicted_seq = torch.Tensor([[predicted_seq]]).to(device)
        
        m_t_fill = [m_t, 20-m_t]

        #for m_y in tqdm(range(72+1), desc='m_y', leave=False):
        for m_y in range(72+1):
            m_y_fill = [m_y]
            if m_y < 72 and m_y !=0:
                m_y_fill.append(144-m_y)
    
            #for m_x in tqdm(range(88+1), desc='m_x',leave=False):
            for m_x in range(88+1):
                m_x_fill = [m_x]
                if m_x < 88 and m_x !=0:
                    m_x_fill.append(176-m_x)

                # Calculation
                result = autocorrelation_motion(predicted_seq=predicted_seq, 
                                                target_seq=input_seq, 
                                                m_x=m_x-88, m_y=m_y-72, m_t=m_t-10,
                                                device=device)
                
                for x in m_x_fill:
                    for y in m_y_fill:
                        for t in m_t_fill:
                            R_xxx[t,y,x] = result
            
    
    print('Finish calculating autocorrelation.')
    print('R_xxx[0,0,0]= ', R_xxx[10,72,88])
                                       
    # ----------------------------------------------------------------
    # Normalize with R_xxx[0,0,0] 
    # Since we have shifted R_xxx, R_xxx[10,72,88] is actually R_xxx[0,0,0]
    # ----------------------------------------------------------------
    _Y = R_xxx

    norm = R_xxx[10,72,88]
    _Y = (_Y/norm)*127.5+127.5
    _Y = np.cast['uint8'](_Y)
    
    # ------------------------
    # Store R_xxx as an video
    # ------------------------
    with open(output_video_name ,'wb') as f:

        width, height, n_frames = 176, 144, 21

        _U = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)
        _V = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)

        #_Y = R_xxx
        for frame in _Y:
            f.write(frame.reshape(-1).astype(np.uint8).tobytes())
            f.write(_U.tobytes())
            f.write(_V.tobytes())
        f.close()
    print('Finish writing ',output_video_name)

if __name__ == '__main__':
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  
    
    yuv_filename = '../MOBILE_352x288_10.yuv'
    output_video_name = './MOBILE_AC_motion.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_Mobile.json'
    
    run_1_3(yuv_filename, output_video_name, 
            dict_filename=dict_filename,
            device=device)
    
    
    yuv_filename = '../AKIYO_352x288_10.yuv'
    output_video_name = './AKIYO_AC_motion.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_AKIYO.json'
    
    run_1_3(yuv_filename, output_video_name, 
            dict_filename=dict_filename,
            device=device)


