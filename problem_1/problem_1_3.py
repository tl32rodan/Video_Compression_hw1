# ## 3.

# +
import numpy as np
import os
import torch
import torch.nn.functional as F
import time
from tqdm.autonotebook import tqdm
import json

from problem_1_2 import read_yuv_video


# -

def autocorrelation_motion(predicted_seq, target_seq, m_x, m_y, m_t, L_x=352, L_y=288, L_t=100):
    '''
        ! Incompatible to autocorrelation(), since this is not "true" autocorrelation calculation !
        
        predicted_seq, target_seq : numpy.ndarray(3d) or torch.Tensor(5d)
            Detail :
                1. predicted_seq is a sequence generated by motion compensation, so it's no need to silce it.
                2. Here we need target_seq also be silced in last dimension (time dimension) already
                
        m_x, m_y, m_z : Scalar
        L_x, L_y, L_z : Scalar
    ''' 
    
    if abs(m_x) >= L_x or abs(m_y) >= L_y or abs(m_t) >= L_t:
        return 0
    
    predicted_seq = predicted_seq[:, :,    :L_t-abs(m_t),   :L_y-abs(m_y),:]
    target_seq    = target_seq[   :, :, abs(m_t):       , abs(m_y):      ,:]
    
    
    C_vvv = (predicted_seq*target_seq).sum().cpu().numpy()
    
    R_xxx = C_vvv / ((L_x-abs(m_x))*(L_y-abs(m_y))*(L_t-abs(m_t)))
    
    #end_t = time.time()
    #print('Autocorrelation value = ',R_xxx,' ; time = ',end_t-start_t)
    return R_xxx


def find_best_fit_block(source_block, target_blocks, unit_size=(4,4), device=torch.device('cpu')):
    '''
    source_block  : torch.Tensor with dimention (unit_size)
    target_blocks : 2D array of torch.Tensor, each element has dimention (unit_size)
    '''
    
    height, width, _, _ = target_blocks.size()
    
    # Calculate MSE
    mse = torch.sum((source_block-target_blocks)**2, (2,3)).view(-1)
    
    _, min_mse_idx = torch.topk(mse, 1, largest=False)
    
    min_mse_idx = min_mse_idx.cpu().numpy()
    
    result_idx = (min_mse_idx//width, min_mse_idx % width)
    #print(' min_mse_idx = ',  min_mse_idx,'; height = ',height, ' width = ',width)
    return result_idx


# +
def motion_compensation_single_reference(source_frame, target_frame, L_x=352, L_y=288,
                                         unit_size=(4,4), 
                                         device=torch.device('cpu'),
                                         predicted_frame = None):
    
    return_result = False
    if predicted_frame == None:
        predicted_frame = torch.zeros_like(source_frame).to(device)
        return_result = True
        
    motion_vector_list = []

    # Construct list of all target blocks to reduce # of ops
    target_blocks = []
    for h in range(L_y-unit_size[0]):
        tmp = []
        for w in range(L_x-unit_size[1]):
            tmp.append(target_frame[h:h+unit_size[0], w:w+unit_size[1]])
        
        target_blocks.append(torch.stack(tmp))
    target_blocks = torch.stack(target_blocks)
    
#     for h_base in tqdm(range(L_y//unit_size[0]), desc='h_base', leave=False):
    for h_base in range(L_y//unit_size[0]):
        for w_base in range(L_x//unit_size[1]):
            # Get current block from source_frame
            source_block = source_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                                        w_base*unit_size[1] : (w_base+1)*unit_size[1]]
            
            
            # Determine margins
            u_margin, d_margin = max(h_base*unit_size[0]-16, 0) , min((h_base+1)*unit_size[0]+15+1, L_y)
            l_margin, r_margin = max(w_base*unit_size[1]-16, 0) , min((w_base+1)*unit_size[1]+15+1, L_x)

            target_subblocks = target_blocks[u_margin:d_margin,l_margin:r_margin]
            
            # Calculate motion compensation result
            
            #print(target_subblocks.size())
            motion_vector = find_best_fit_block(source_block = source_block,
                                                target_blocks = target_subblocks,
                                                unit_size = unit_size,
                                                device=device)
            if motion_vector[0] >= target_subblocks.size()[0] or motion_vector[1] >= target_subblocks.size()[1]:
                print('motion_vector = ',motion_vector)
                print('target_subblocks.size() = ', target_subblocks.size())
            
            predicted_block = target_subblocks[motion_vector[0],motion_vector[1],:,:][0]
                        
#             if predicted_block.size() != (4,4):
#                 print(u_margin,d_margin,l_margin,r_margin)
#                 print('predicted_block.size() =',predicted_block.size())
#                 print('motion_vector = ',motion_vector)
#                 print('-------------------')
                
            motion_vector = (motion_vector[0]-(h_base*unit_size[0]-u_margin), 
                             motion_vector[1]-(w_base*unit_size[1]-l_margin))

            # Paste prediction onto its own location
            predicted_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                            w_base*unit_size[1] : (w_base+1)*unit_size[1]] = predicted_block

            # Record motion vector
            motion_vector_list.append(motion_vector)
    if return_result:
        return predicted_block, motion_vector_list
    else:
        return motion_vector_list


# -

def run_1_3(yuv_filename, output_video_name):
    Y, U, V = read_yuv_video(yuv_filename)
    
    #device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') 
    device = torch.device('cpu')
    input_seq = torch.Tensor(Y).type(torch.float64).to(device)

    L_x, L_y, L_t = 352, 288, 100
    unit_size=(4,4) # block size ; (height, width)
    search_range = (16,16) # Motion compensation search range : -16~+15

    # First, do motion compensation
    motion_result_dict_by_step = {}

    for i in tqdm(range(11), desc='m_t'):        
        motion_vector_lists = []
        predicted_frames = torch.zeros_like(input_seq[:L_t -i])
        
        for source_frame_idx in tqdm(range(L_t -i), desc='source frame index', leave=False):
            source_frame = input_seq[source_frame_idx]
            target_frame = input_seq[source_frame_idx + i]

            motion_vector_list = motion_compensation_single_reference(source_frame, target_frame, 
                                                                        L_x=L_x, L_y=L_y, 
                                                                        unit_size=unit_size,
                                                                        device=device,
                                                                        predicted_frame=predicted_frames[source_frame_idx])

            motion_vector_lists.append(motion_vector_list)
        
        
        motion_result_dict_by_step[i] = {
            'Predict_frames':predicted_frames,
            'Motion_vector_lists':motion_vector_lists
        }
    
    return motion_result_dict_by_step        
    
    
    # Second, calculate autocorrelation with motion compensated frames
    ##### TODO #####

if __name__ == '__main__':
    yuv_filename = '../AKIYO_352x288_10.yuv'
    output_video_name = './AKIYO_AC_motion.yuv'
    motion_result_dict = run_1_3(yuv_filename, output_video_name)



    print(motion_result_dict[0]['Motion_vector_lists'][0][0])

    # Save results
    with open('./motion_ckpt/motion_result_dict_Mobile.json','w') as f:
        tmp_dict = motion_result_dict

        for key in tmp_dict:
            tmp_dict[key]['Predict_frames'] = tmp_dict[key]['Predict_frames'].cpu().numpy().tolist()
    #         tmp_dict[key]['Motion_vector_lists'] = [torch.stack(l) for l in tmp_dict[key]['Motion_vector_lists']]
    #         tmp_dict[key]['Motion_vector_lists'] = torch.stack(tmp_dict[key]['Motion_vector_lists'])
    #         tmp_dict[key]['Motion_vector_lists'] = tmp_dict[key]['Motion_vector_lists'].cpu().numpy().tolist()

        json.dump(tmp_dict, f)


