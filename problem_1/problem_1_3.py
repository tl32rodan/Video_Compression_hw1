# ## 3.

# +
import numpy as np
import os
import torch
import torch.nn.functional as F
import time
from tqdm.autonotebook import tqdm
import json
import copy

from problem_1_2 import read_yuv_video


# +
def autocorrelation_motion(predicted_seq, target_seq, m_x, m_y, m_t, 
                           L_x=352, L_y=288, L_t=100,
                           device=torch.device('cpu')):
    '''
        ! Incompatible to autocorrelation(), since this is not a "true" autocorrelation calculation !
        
        predicted_seq, target_seq : torch.Tensor(5d)
            Detail :
                1. predicted_seq is a sequence generated by motion compensation, 
                   so it's no need to be sliced w.r.t time.

                
        m_x, m_y, m_t : Scalar
        L_x, L_y, L_t : Scalar
    ''' 
    
    if abs(m_x) >= L_x or abs(m_y) >= L_y or abs(m_t) >= L_t:
        return 0
    
    predicted_seq = predicted_seq[:, :,        : ,         :L_y-abs(m_y),        :L_x-abs(m_x)]
    target_seq    =    target_seq[:, :,abs(m_t): , abs(m_y):            ,abs(m_x):            ]
    

#     print(predicted_seq.size())
#     print(target_seq.size())
    C_vvv = (predicted_seq*target_seq).sum().cpu().numpy()
    
    R_xxx = C_vvv / ((L_x-abs(m_x))*(L_y-abs(m_y))*(L_t-abs(m_t)))
    
    #end_t = time.time()
    #print('Autocorrelation value = ',R_xxx,' ; time = ',end_t-start_t)
    return R_xxx


# -

def find_best_fit_block(source_block, target_blocks, unit_size=(4,4), device=torch.device('cpu')):
    '''
    source_block  : torch.Tensor with dimention (unit_size)
    target_blocks : 2D array of torch.Tensor, each element has dimention (unit_size)
    '''
    
    height, width, _, _ = target_blocks.size()
    
    # Calculate MSE
    mse = torch.sum((source_block-target_blocks)**2, (2,3)).view(-1)
    
    _, min_mse_idx = torch.topk(mse, 1, largest=False)
    
    min_mse_idx = min_mse_idx.cpu().numpy()
    # view(-1) is row major ; result_idx = (h, w)
    result_idx = ((min_mse_idx//width).item(), (min_mse_idx % width).item())
    
    return result_idx


def motion_compensation_single_reference(source_frame, target_frame,
                                         L_x=352, L_y=288,
                                         unit_size=(4,4), 
                                         device=torch.device('cpu'),
                                         predicted_frame = None):
    
    return_result = False
    if predicted_frame == None:
        predicted_frame = torch.zeros_like(source_frame).to(device)
        return_result = True
        
    motion_vector_list = []

    # Construct list of all target blocks to reduce # of ops
    target_blocks = []
    for h in range(L_y-unit_size[0]):
        tmp = []
        for w in range(L_x-unit_size[1]):
            tmp.append(target_frame[h:h+unit_size[0], w:w+unit_size[1]])
        
        target_blocks.append(torch.stack(tmp))
    target_blocks = torch.stack(target_blocks)
    
    # Calculate motion compesation for all blocks
    for h_base in range(L_y//unit_size[0]):
        for w_base in range(L_x//unit_size[1]):
            # Get current block from source_frame
            source_block = source_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                                        w_base*unit_size[1] : (w_base+1)*unit_size[1]]
            
            
            # Determine margins
            u_margin, d_margin = max(h_base*unit_size[0]-16, 0) , min(h_base*unit_size[0]+15, L_y)
            l_margin, r_margin = max(w_base*unit_size[1]-16, 0) , min(w_base*unit_size[1]+15, L_x)

            target_subblocks = target_blocks[u_margin:d_margin,l_margin:r_margin]
            
            # Calculate motion compensation result
            
            motion_vector = find_best_fit_block(source_block=source_block,
                                                target_blocks=target_subblocks,
                                                unit_size=unit_size,
                                                device=device)
            
            predicted_block = target_subblocks[motion_vector[0],motion_vector[1]]
                
            motion_vector = (motion_vector[0]-(h_base*unit_size[0]-u_margin), 
                             motion_vector[1]-(w_base*unit_size[1]-l_margin))

            # Paste prediction onto its own location
            predicted_frame[h_base*unit_size[0] : (h_base+1)*unit_size[0],
                            w_base*unit_size[1] : (w_base+1)*unit_size[1]] = predicted_block

            # Record motion vector
            motion_vector_list.append(motion_vector)
            
    if return_result:
        return predicted_frame, motion_vector_list
    else:
        return motion_vector_list


def establish_motion_compensation_dict(Y, output_video_name, dict_filename='tmp.json',
                                       unit_size=(4,4), # block size ; (height, width)
                                       device = torch.device('cpu')):
   
    input_seq = torch.Tensor(Y).type(torch.float64).to(device)

    L_x, L_y, L_t = 352, 288, 100
    search_range = (16,16) # Motion compensation search range : -16~+15

    motion_result_dict_by_step = {}

    for i in tqdm(range(11), desc='m_t'):        
        motion_vector_lists = []
        predicted_frames = torch.zeros_like(input_seq[:L_t -i])
        
        for idx in tqdm(range(i, L_t), desc='idx', leave=False):
            
            # *Poor naming here
            source_frame = input_seq[idx]
            target_frame = input_seq[idx - i]

            motion_vector_list = motion_compensation_single_reference(source_frame, target_frame, 
                                                                        L_x=L_x, L_y=L_y, 
                                                                        unit_size=unit_size,
                                                                        device=device,
                                                                        predicted_frame=predicted_frames[idx - i])

            motion_vector_lists.append(motion_vector_list)
        
        
        motion_result_dict_by_step[str(i)] = {
            'Predict_frames':predicted_frames.cpu().numpy(),
            'Motion_vector_lists':motion_vector_lists
        }
        
    # Save results
    with open(dict_filename,'w') as f:
        tmp_dict = copy.deepcopy(motion_result_dict_by_step)

        for key in tmp_dict:
            tmp_dict[key]['Predict_frames'] = tmp_dict[key]['Predict_frames'].astype('int').tolist()

        json.dump(tmp_dict, f)
        f.close()
    
    return motion_result_dict_by_step        


def normalize_seq(Y):
    Y = np.array(Y)
    _Y = []
    for frame in Y:
        _Y.append(frame-frame.mean())

    return np.array(_Y)


# +
def run_1_3(yuv_filename, output_video_name, dict_filename='tmp.json', unit_size=(4,4), device=torch.device('cpu')):
    Y, U, V = read_yuv_video(yuv_filename)
           
    R_xxx = np.zeros([21,144,176])
    
    input_seq = torch.Tensor([[np.array(Y)]]).type(torch.float32).to(device)
    
    # ----------------------------------------------------------------
    # First, do motion compensation if the dict is not established
    # ----------------------------------------------------------------
    if not os.path.isfile(dict_filename): 
        # Establish new dict
        print('Start to establish new dict of motion-compensated frames...')
        motion_result_dict = establish_motion_compensation_dict(Y=Y, 
                                                                output_video_name=output_video_name, 
                                                                unit_size=unit_size,
                                                                dict_filename=dict_filename,
                                                                device=device)
        print('Finish estiblishment')
    else:
        # Use exist dict
        
        print('Start to load exist dict of motion-compensated frames...')
        with open(dict_filename, 'r') as fp: 
            motion_result_dict = json.load(fp)
            fp.close()
        print('Finish loading')    
        
#     print('motion_result_dict.keys() = ',motion_result_dict.keys())
    # ----------------------------------------------------------------
    # Second, calculate autocorrelation with motion compensated frames
    # Normalize each frame with its own mean before calculating autocorrelation
    # ----------------------------------------------------------------
    Y = normalize_seq(Y)
    input_seq = torch.Tensor([[Y]]).type(torch.float32).to(device)
    
    print('Start calculating autocorrelation...')
    for m_t in tqdm(range(10+1), desc='m_t'):
        
        # Get pre-calculated compensated frames
        predicted_seq = motion_result_dict[str(abs(m_t-10))]['Predict_frames']
        predicted_seq = normalize_seq(predicted_seq) # Normalize MC video
        predicted_seq = torch.Tensor([[predicted_seq]]).to(device)
        
        m_t_fill = [m_t, 20-m_t]

        #for m_y in tqdm(range(72+1), desc='m_y', leave=False):
        for m_y in range(72+1):
            m_y_fill = [m_y]
            if m_y < 72 and m_y !=0:
                m_y_fill.append(144-m_y)
    
            #for m_x in tqdm(range(88+1), desc='m_x',leave=False):
            for m_x in range(88+1):
                m_x_fill = [m_x]
                if m_x < 88 and m_x !=0:
                    m_x_fill.append(176-m_x)

                # Calculation
                result = autocorrelation_motion(predicted_seq=predicted_seq, 
                                                target_seq=input_seq, 
                                                m_x=m_x-88, m_y=m_y-72, m_t=m_t-10,
                                                device=device)
                
                for x in m_x_fill:
                    for y in m_y_fill:
                        for t in m_t_fill:
                            R_xxx[t,y,x] = result
            
    
    print('Finish calculating autocorrelation')
    print('R_xxx[0,0,0]= ', R_xxx[10,72,88])
                                       
    # ----------------------------------------------------------------
    # Normalize with R_xxx[0,0,0] 
    # Since we have shifted R_xxx, R_xxx[10,72,88] is actually R_xxx[0,0,0]
    # ----------------------------------------------------------------
    _Y = R_xxx

    norm = R_xxx[10,72,88]
    _Y = (_Y/norm)*127.5+127.5
    _Y = np.cast['uint8'](_Y)
    
    # ------------------------
    # Store R_xxx as an video
    # ------------------------
    with open(output_video_name ,'wb') as f:

        width, height, n_frames = 176, 144, 21

        _U = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)
        _V = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)

        #_Y = R_xxx
        for frame in _Y:
            f.write(frame.reshape(-1).astype(np.uint8).tobytes())
            f.write(_U.tobytes())
            f.write(_V.tobytes())
        f.close()
    print('Finish writing ',output_video_name)

# +
import json

def save_yuv_from_motion_dict(dict_filename = './motion_ckpt/motion_result_dict_Mobile_4x4.json',
                              m_t_value = 1,
                              output_video_name = './MOBILE_mt_10.yuv' ):

    with open(dict_filename, 'r') as fp: 
        motion_result_dict = json.load(fp)
        fp.close()

    Y = np.array(motion_result_dict[str(m_t_value)]['Predict_frames'])

    with open(output_video_name ,'wb') as f:

        n_frames, height, width  = Y.shape

        _U = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)
        _V = (np.ones((width*height//4))*128).reshape(-1).astype(np.uint8)

        #_Y = R_xxx
        for frame in Y:
            f.write(frame.reshape(-1).astype(np.uint8).tobytes())
            f.write(_U.tobytes())
            f.write(_V.tobytes())
        f.close()
        print('Finish writing ',output_video_name)


# -

if __name__ == '__main__':
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  
    
    yuv_filename = '../MOBILE_352x288_10.yuv'
    output_video_name = './MOBILE_AC_motion_4x4.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_Mobile_4x4.json'
    
    run_1_3(yuv_filename, output_video_name, 
            unit_size=(4,4),
            dict_filename=dict_filename,
            device=device)
    
    
    yuv_filename = '../AKIYO_352x288_10.yuv'
    output_video_name = './AKIYO_AC_motion_4x4.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_AKIYO_4x4.json'
    
    run_1_3(yuv_filename, output_video_name, 
            unit_size=(4,4),
            dict_filename=dict_filename,
            device=device)
    
    
    
    yuv_filename = '../MOBILE_352x288_10.yuv'
    output_video_name = './MOBILE_AC_motion_16x16.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_Mobile_16x16.json'
    
    run_1_3(yuv_filename, output_video_name, 
            unit_size=(16,16),
            dict_filename=dict_filename,
            device=device)
    
    
    yuv_filename = '../AKIYO_352x288_10.yuv'
    output_video_name = './AKIYO_AC_motion_16x16.yuv'
    dict_filename = './motion_ckpt/motion_result_dict_AKIYO_16x16.json'
    
    run_1_3(yuv_filename, output_video_name, 
            unit_size=(16,16),
            dict_filename=dict_filename,
            device=device)



    dict_filename = './motion_ckpt/motion_result_dict_AKIYO_4x4.json'
    m_t_value = 1
    output_video_name = './AKIYO_mt_1_4x4.yuv'

    save_yuv_from_motion_dict(dict_filename, m_t_value, output_video_name)



    dict_filename = './motion_ckpt/motion_result_dict_AKIYO_16x16.json'
    m_t_value = 10
    output_video_name = './AKIYO_mt_10_16x16.yuv'

    save_yuv_from_motion_dict(dict_filename, m_t_value, output_video_name)
